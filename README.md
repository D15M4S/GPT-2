# GPT-2 Implementation from Scratch

A hands-on implementation of GPT-2 for learning and understanding transformer architecture step by step.

## Overview

This project is a personal learning journey to deeply understand the GPT-2 architecture by implementing it from scratch. Rather than just using pre-built libraries, the goal is to build each component manually to gain a thorough understanding of how large language models work.

## Paper Reference

This implementation is based on the original GPT-2 paper:

**"Language Models are Unsupervised Multitask Learners"**
*Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever*
OpenAI, 2019

- [Paper (PDF)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [OpenAI Blog Post](https://openai.com/research/better-language-models)
- [Official GPT-2 GitHub Repository](https://github.com/openai/gpt-2)

## Learning Goals

- Understand the Transformer decoder architecture
- Implement multi-head self-attention mechanism
- Learn about positional encoding and token embeddings
- Explore layer normalization and residual connections
- Practice building neural networks from foundational components

## Project Status

ðŸš§ **Work in Progress** - This project is being built incrementally as a learning exercise.

## License

MIT
